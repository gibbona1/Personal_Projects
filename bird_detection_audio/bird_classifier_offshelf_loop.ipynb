{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6db40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7bf22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, auc\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from data_set_params import DataSetParams\n",
    "from scipy.io import wavfile\n",
    "params = DataSetParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f2b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443712a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! unzip ~/richfield_birds_split.zip -d ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c155b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d372333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Common Buzzard',\n",
       " 'Common Kestrel',\n",
       " 'Common Snipe',\n",
       " 'Eurasian Curlew',\n",
       " 'European Herring Gull',\n",
       " 'European Robin',\n",
       " 'Meadow Pipit',\n",
       " 'Mute Swan',\n",
       " 'Northern Lapwing',\n",
       " 'Rook',\n",
       " 'Tundra Swan',\n",
       " 'Tundra Swan (Bewicks)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_dir = pathlib.Path('/root/tensorflow_datasets/downloads/extracted/TAR_GZ.opihi.cs.uvic.ca_sound_music_speechbya81rFcWfLSW6ey5cynqyeq2qiePcL-7asMoNO6IQ0.tar.gz/music_speech')\n",
    "data_dir   = 'richfield_birds_split'#'dublin_dl_birds_split'#\n",
    "categories = np.array(tf.io.gfile.listdir(data_dir))\n",
    "categories = [category for category in categories if 'wav' not in category]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a230e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "\n",
    "    # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
    "    # to work in a TensorFlow graph.\n",
    "    return parts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489e0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "#filenames = tf.io.gfile.glob('birds/*/*')\n",
    "filenames = [filename for filename in filenames if 'wav' in filename]\n",
    "filenames = tf.random.shuffle(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42367d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "batch_size = 32\n",
    "EPOCHS     = 1#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6c4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247c71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f2f66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, top_k_accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from math import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b75f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model, x_test, y_true, name, filename_run):\n",
    "        model.save('models/'+filename_run+'.h5')\n",
    "    \n",
    "        pred_lists = model.predict(x_test)\n",
    "        y_pred     = np.argmax(pred_lists, axis=-1)\n",
    "        pred_df    = pd.DataFrame(pred_lists, columns = categories)\n",
    "        \n",
    "        softmax_prediction_df = pred_df.apply(lambda x: np.exp(x - np.max(x))/np.exp(x - np.max(x)).sum(), axis=1)\n",
    "        softmax_prediction_df.to_csv('results/'+filename_run+'softmax_prediction_df.csv')\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        fig = plotly_cm(cm, categories)\n",
    "        fig.write_html('results/'+filename_run+'confusion_matrix.html')\n",
    "        \n",
    "        num_trainable    = sum([prod(w.shape) for w in model.trainable_weights])\n",
    "        num_nontrainable = sum([prod(w.shape) for w in model.non_trainable_weights])\n",
    "        \n",
    "        onehot_data = OneHotEncoder(sparse=False)\n",
    "        onehot_data = onehot_data.fit_transform(np.array(y_true).reshape(len(y_true),1))\n",
    "        roc_auc = [0]*num_classes\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            roc_auc[i] = roc_auc_score(onehot_data[:, i], softmax_prediction_df.to_numpy()[:, i])\n",
    "        \n",
    "        name_df = pd.DataFrame(data={\n",
    "                  'model':     name}, index=[0])\n",
    "        metric_df = pd.DataFrame(data={\n",
    "                  'top_1_acc': [accuracy_score(y_pred, y_true)],\n",
    "                  'top_5_acc': [top_k_accuracy_score(y_true, softmax_prediction_df, k=5)],\n",
    "                  'precision': [precision_score(y_pred, y_true, average = 'weighted')], \n",
    "                  'f1':        [f1_score(y_pred, y_true, average = 'weighted')]\n",
    "                 })\n",
    "        param_df = pd.DataFrame(data={\n",
    "                  'trainable_params': [num_trainable],\n",
    "                  'nontrainable_params': [num_nontrainable]\n",
    "                 })\n",
    "        auc_df = pd.DataFrame([roc_auc], columns = ['auc_'+categories[i].replace(' ', '') for i in range(num_classes)])\n",
    "        \n",
    "        metric_df =  pd.concat([name_df, metric_df],axis=1)\n",
    "        metric_df.to_csv('results/'+filename_run+'metric_df.csv')\n",
    "        \n",
    "        param_df  =  pd.concat([name_df, param_df],axis=1)\n",
    "        param_df.to_csv('results/'+filename_run+'param_df.csv')\n",
    "        \n",
    "        auc_df    =  pd.concat([name_df, auc_df],axis=1)\n",
    "        auc_df.to_csv('results/'+filename_run+'auc_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3795af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg19(input_shape):\n",
    "    vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    vgg_model.trainable = False ## Not trainable weights\n",
    "    #vgg_model.summary()\n",
    "    x = vgg_model.output\n",
    "    x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x) # Softmax for multiclass\n",
    "    transfer_vgg_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "    transfer_vgg_model.compile(\n",
    "        optimizer = optimizers.Adam(learning_rate=0.0001),\n",
    "        loss      = losses.SparseCategoricalCrossentropy(),\n",
    "        metrics   = 'accuracy'\n",
    "        )\n",
    "    return transfer_vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5f530d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet50(input_shape):\n",
    "    resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    resnet_model.trainable = False ## Not trainable weights\n",
    "    #resnet_model.summary()\n",
    "    x = resnet_model.output\n",
    "    x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x) # Softmax for multiclass\n",
    "    transfer_resnet_model = Model(inputs=resnet_model.input, outputs=x)\n",
    "    transfer_resnet_model.compile(\n",
    "        optimizer = optimizers.Adam(learning_rate=0.0001),\n",
    "        loss      = losses.SparseCategoricalCrossentropy(),\n",
    "        metrics   = 'accuracy'\n",
    "        )\n",
    "    return transfer_resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1304621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_model_run(filenames, index):\n",
    "    print(\"Index: \", index)\n",
    "    \n",
    "    filenames   = tf.random.shuffle(filenames)\n",
    "    all_labs    = [get_label(y).numpy().decode() for y in filenames]\n",
    "    filename_df = pd.DataFrame({'name': filenames.numpy(),\n",
    "                                'label': all_labs})\n",
    "    \n",
    "    train, test = train_test_split(filename_df, test_size=0.2, stratify=filename_df[['label']])\n",
    "    train_files = tf.random.shuffle(train['name'])\n",
    "    test_files  = tf.random.shuffle(test['name'])\n",
    "\n",
    "    def concat_xy(ds):\n",
    "            x_tmp  = [x for x,_ in ds]\n",
    "            x_tmp  = tf.stack(x_tmp)\n",
    "            xs_tmp = tf.unstack(x_tmp, axis=-1)\n",
    "            xs_tmp = [tf.expand_dims(x_ind, axis=-1) for x_ind in xs_tmp]\n",
    "            y      = np.array([y for _,y in ds])\n",
    "            return xs_tmp, y\n",
    "    \n",
    "    print('Getting data')\n",
    "    choices  = ['Mod']\n",
    "    train_ds = preprocess_dataset(train_files, choices, categories, req_width=750, single_to_rgb = True, resize = 4)\n",
    "    test_ds  = preprocess_dataset(test_files,  choices, categories, req_width=750, single_to_rgb = True, resize = 4)\n",
    "    \n",
    "    choices = ['AbsRe', 'AbsIm', 'Ang', 'Mod']\n",
    "    train_ds_mult = preprocess_dataset(train_files, choices, categories, req_width=750, resize = 4)\n",
    "    test_ds_mult  = preprocess_dataset(test_files,  choices, categories, req_width=750, resize = 4)\n",
    "    \n",
    "    X_train, y_train = concat_xy(train_ds_mult)\n",
    "    X_test,  y_test  = concat_xy(test_ds_mult)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    num_channels = len(X_train)\n",
    "    concat_shape = X_train[0].shape[1:]\n",
    "    \n",
    "    for spec, _ in train_ds.take(1):\n",
    "        input_shape = spec.shape\n",
    "        \n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    test_ds  = test_ds.batch(batch_size)\n",
    "    train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "    test_ds  = test_ds.cache().prefetch(AUTOTUNE)\n",
    "    \n",
    "    filename_idx = datetime.now().strftime(\"%Y%m%d-%H%M%S\").replace('-', '_')+'_'+data_dir+'_'+str(index)\n",
    "    \n",
    "    if not os.path.isdir('results'):\n",
    "        os.mkdir('results')\n",
    "    \n",
    "    np.save('results/'+filename_idx+'_filenames.npy', filenames.numpy())\n",
    "    \n",
    "    #this will save the model performing best on val accuracy\n",
    "    def best_model_cp():\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"best_model\",\n",
    "            monitor = \"val_accuracy\",\n",
    "            mode    = \"max\",\n",
    "            save_best_only = True,\n",
    "            save_weights_only = True)\n",
    "        return checkpoint\n",
    "    \n",
    "    ## Load and run models\n",
    "    \n",
    "    #VGG19\n",
    "    print(\"VGG19\")\n",
    "    model    = load_vgg19(input_shape)\n",
    "    model_name   = 'vgg19'\n",
    "    filename_run = filename_idx+'_'+model_name\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data = test_ds,\n",
    "                        callbacks       = [best_model_cp()],\n",
    "                        epochs          = EPOCHS)\n",
    "    \n",
    "    pd.DataFrame(history.history).to_csv('results/'+filename_run+'_model_history.csv')\n",
    "    \n",
    "    model.load_weights(\"best_model\")\n",
    "    \n",
    "    save_results(model, test_ds, y_test, model_name, filename_run)\n",
    "\n",
    "    #ResNet50\n",
    "    print(\"ResNet50\")\n",
    "    model = load_resnet50(input_shape)\n",
    "    model_name   = 'resnet50'\n",
    "    filename_run = filename_idx+'_'+model_name\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data = test_ds,\n",
    "                        callbacks       = [best_model_cp()],\n",
    "                        epochs          = EPOCHS)\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv('results/'+filename_run+'_model_history.csv')\n",
    "    \n",
    "    model.load_weights(\"best_model\")\n",
    "\n",
    "    save_results(model, test_ds, y_test, model_name, filename_run)\n",
    "\n",
    "    #small_cnn\n",
    "    print(\"Small CNN\")\n",
    "    model  = main_cnn(input_shape, num_classes)\n",
    "    model_name   = 'smallcnn'\n",
    "    filename_run = filename_idx+'_'+model_name\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data = test_ds,\n",
    "                        callbacks       = [best_model_cp()],\n",
    "                        epochs          = EPOCHS)\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv('results/'+filename_run+'_model_history.csv')\n",
    "    \n",
    "    model.load_weights(\"best_model\")\n",
    "\n",
    "    save_results(model, test_ds, y_test, model_name, filename_run)\n",
    "    \n",
    "    #concat\n",
    "    print(\"Concat\")\n",
    "    model      = concat_model(concat_shape, num_channels, num_classes)\n",
    "    model_name   = 'concat'\n",
    "    filename_run = filename_idx+'_'+model_name\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        callbacks       = [best_model_cp()],\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = batch_size)\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv('results/'+filename_run+'_model_history.csv')\n",
    "    \n",
    "    model.load_weights(\"best_model\")\n",
    "    \n",
    "    save_results(model, X_test, y_test, model_name, filename_run)\n",
    "\n",
    "    #concat2\n",
    "    print(\"Concat2\")\n",
    "    model     = concat_model2(concat_shape, num_channels, num_classes)\n",
    "    model_name   = 'concat2'\n",
    "    filename_run = filename_idx+'_'+model_name\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        callbacks       = [best_model_cp()],\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = batch_size)\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv('results/'+filename_run+'_model_history.csv')\n",
    "    \n",
    "    model.load_weights(\"best_model\")\n",
    "    \n",
    "    save_results(model, X_test, y_test, model_name, filename_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ce69b",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1524e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a65dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0\n",
      "Getting data\n",
      "Done\n",
      "VGG19\n",
      "ResNet50\n",
      "71/71 [==============================] - 105s 1s/step - loss: 2.0892 - accuracy: 0.2953 - val_loss: 1.7501 - val_accuracy: 0.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning:\n",
      "\n",
      "Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small CNN\n",
      "71/71 [==============================] - 14s 198ms/step - loss: 2.6032 - accuracy: 0.1536 - val_loss: 2.3891 - val_accuracy: 0.2570\n",
      "Concat\n",
      "71/71 [==============================] - 56s 784ms/step - loss: 2.3790 - accuracy: 0.2535 - val_loss: 2.0783 - val_accuracy: 0.4243\n",
      "Concat2\n",
      "71/71 [==============================] - 63s 872ms/step - loss: 2.5813 - accuracy: 0.1417 - val_loss: 2.4126 - val_accuracy: 0.2606\n",
      "Time so far: 413.64055919647217\n",
      "Index:  1\n",
      "Getting data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-74d05141fc16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain_model_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time so far:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-f413305988e3>\u001b[0m in \u001b[0;36mmain_model_run\u001b[1;34m(filenames, index)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mtest_ds_mult\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mchoices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds_mult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0my_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mconcat_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds_mult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-f413305988e3>\u001b[0m in \u001b[0;36mconcat_xy\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconcat_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mx_tmp\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mx_tmp\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mxs_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-f413305988e3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconcat_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mx_tmp\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mx_tmp\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mxs_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2720\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2721\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2722\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2723\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNext\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2724\u001b[0m         \"output_shapes\", output_shapes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    main_model_run(filenames, i)\n",
    "    print(\"Time so far:\", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc963a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(res_df_t.to_latex(bold_rows = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9fb407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  1\n",
      "Getting data\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "print(\"Index: \", index)\n",
    "\n",
    "filenames   = tf.random.shuffle(filenames)\n",
    "all_labs    = [get_label(y).numpy().decode() for y in filenames]\n",
    "filename_df = pd.DataFrame({'name': filenames.numpy(),\n",
    "                            'label': all_labs})\n",
    "\n",
    "train, test = train_test_split(filename_df, test_size=0.2, stratify=filename_df[['label']])\n",
    "train_files = tf.random.shuffle(train['name'])\n",
    "test_files  = tf.random.shuffle(test['name'])\n",
    "\n",
    "def concat_xy(ds):\n",
    "        x_tmp  = [x for x,_ in ds]\n",
    "        x_tmp  = tf.stack(x_tmp)\n",
    "        xs_tmp = tf.unstack(x_tmp, axis=-1)\n",
    "        xs_tmp = [tf.expand_dims(x_ind, axis=-1) for x_ind in xs_tmp]\n",
    "        y      = np.array([y for _,y in ds])\n",
    "        return xs_tmp, y\n",
    "\n",
    "print('Getting data')\n",
    "choices  = ['Mod']\n",
    "train_ds = preprocess_dataset(train_files, choices, categories, req_width=750, single_to_rgb = True, resize = 4)\n",
    "test_ds  = preprocess_dataset(test_files,  choices, categories, req_width=750, single_to_rgb = True, resize = 4)\n",
    "\n",
    "#choices = ['AbsRe', 'AbsIm', 'Ang', 'Mod']\n",
    "#train_ds_mult = preprocess_dataset(train_files, choices, categories, req_width=750, resize = 4)\n",
    "#test_ds_mult  = preprocess_dataset(test_files,  choices, categories, req_width=750, resize = 4)\n",
    "\n",
    "#X_train, y_train = concat_xy(train_ds_mult)\n",
    "#X_test,  y_test  = concat_xy(test_ds_mult)\n",
    "print(\"Done\")\n",
    "\n",
    "num_channels = len(X_train)\n",
    "concat_shape = X_train[0].shape[1:]\n",
    "\n",
    "for spec, _ in train_ds.take(1):\n",
    "    input_shape = spec.shape\n",
    "\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "test_ds  = test_ds.batch(batch_size)\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "test_ds  = test_ds.cache().prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5dfcdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 14s 602ms/step - loss: 2.6678 - accuracy: 0.0651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6678380966186523, 0.06514084339141846]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = main_cnn(input_shape, num_classes)\n",
    "model_name   = 'smallcnn'\n",
    "#filename_run = filename_idx+'_'+model_name\n",
    "\n",
    "model.evaluate(test_ds)\n",
    "#history = model.fit(train_ds,\n",
    "#                    validation_data = test_ds,\n",
    "#                    callbacks       = [best_model_cp()],\n",
    "#                    epochs          = EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
